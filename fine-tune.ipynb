{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import cohen_kappa_score,mean_absolute_error,mean_squared_error,accuracy_score,explained_variance_score,r2_score,confusion_matrix,ConfusionMatrixDisplay,classification_report,f1_score, silhouette_score,adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model': 'model/deberta-v3-large',\n",
    "    'dropout': 0.5,\n",
    "    'max_length': 512,\n",
    "    'batch_size': 16,\n",
    "    'epochs': 5,\n",
    "    'freeze_lr': 1e-5,\n",
    "    'device': 'cuda' ,\n",
    "    'scheduler': 'StepLR'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data/asap-aes/training_set_rel3.xls')\n",
    "data = data[data['essay_set']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "score2label =dict()\n",
    "label = 0\n",
    "for i in sorted(list(set((data[data['essay_set']==1]['domain1_score'])))):\n",
    "    score2label[i] = label\n",
    "    label += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2score = {score2label[i]:i for i in score2label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['domain1_score'] = [score2label[i] for i in data['domain1_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EssayDataset:\n",
    "    def __init__(self, df, config, tokenizer=None, is_test=False):\n",
    "        # 将输入的DataFrame进行重置索引，并保存在对象的成员变量df中\n",
    "        self.df = df\n",
    "        self.classes = 'domain1_score'\n",
    "        self.max_len = config['max_length']\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test \n",
    "    def __getitem__(self, idx):\n",
    "        # 获取指定索引对应的原始文本\n",
    "        sample = self.df['essay'][idx]\n",
    "        # 使用tokenizer对原始文本进行编码，得到包含input_ids, token_type_ids和attention_mask的tokenized字典\n",
    "        tokenized = self.tokenizer.encode_plus(sample,\n",
    "                                               None,\n",
    "                                               add_special_tokens=True,\n",
    "                                               max_length=self.max_len,\n",
    "                                               truncation=True,\n",
    "                                               padding='max_length'\n",
    "                                              )\n",
    "        \n",
    "        # 构造inputs字典，将编码后的tokens转换为PyTorch张量\n",
    "        inputs = {\n",
    "            \"input_ids\": torch.tensor(tokenized['input_ids'], dtype=torch.long),\n",
    "            #\"token_type_ids\": torch.tensor(tokenized['token_type_ids'], dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(tokenized['attention_mask'], dtype=torch.long)\n",
    "        }\n",
    "        \n",
    "        # 如果是测试数据集，直接返回inputs字典\n",
    "        if self.is_test == True:\n",
    "            return inputs\n",
    "        \n",
    "        # 否则，获取对应样本的标签，并构造targets字典，将标签转换为PyTorch张量\n",
    "        label = self.df.loc[idx, self.classes]\n",
    "        targets = {\n",
    "            \"labels\": torch.tensor(label,dtype=torch.long),\n",
    "        }\n",
    "        \n",
    "        return inputs, targets\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "traind = data[data['essay_set'] == 1][:1400]\n",
    "testd = data[data['essay_set'] == 1][1400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('model/tokenizer/deberta-v3-large/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainds = EssayDataset(traind, config, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "testds = EssayDataset(testd, config, tokenizer=tokenizer, is_test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(trainds,\n",
    "                                           batch_size=config['batch_size'],\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=0,\n",
    "                                           pin_memory=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrozenEssayModel(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(FrozenEssayModel,self).__init__()\n",
    "        self.model_name = config['model']\n",
    "        self.encoder = AutoModel.from_pretrained(self.model_name)\n",
    "        \n",
    "        # this is how you freeze a model: the base_model is generic term for the transformer name\n",
    "        for param in self.encoder.base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        self.fc1 = nn.Linear(self.encoder.config.hidden_size,64)\n",
    "        #self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64,11)\n",
    "        \n",
    "    def forward(self,inputs):\n",
    "        outputs = self.encoder(**inputs, return_dict=False)\n",
    "        outputs = self.dropout(outputs[0][:, -1])\n",
    "        outputs = self.fc1(outputs)\n",
    "        #outputs = torch.nn.ReLU()(outputs)\n",
    "        #outputs = self.dropout(outputs)\n",
    "        outputs = self.fc2(outputs)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, loaders, config,lr='unfreeze'):\n",
    "        self.model = model\n",
    "        self.train_loader = loaders\n",
    "        self.config = config\n",
    "        self.input_keys = ['input_ids','attention_mask']\n",
    "        self.loss_fn = nn.CrossEntropyLoss()\n",
    "        \n",
    "        if lr == 'unfreeze':\n",
    "            self.lr = self.config['unfreeze_lr']\n",
    "        else:\n",
    "            self.lr = self.config['freeze_lr']\n",
    "            \n",
    "        self.optim = self._get_optim()\n",
    "        \n",
    "        self.scheduler_options = {\n",
    "            'CosineAnnealingWarmRestarts': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(self.optim, T_0=5,eta_min=1e-7),\n",
    "            'ReduceLROnPlateau': torch.optim.lr_scheduler.ReduceLROnPlateau(self.optim, 'min', min_lr=1e-7),\n",
    "            'StepLR': torch.optim.lr_scheduler.StepLR(self.optim,step_size=2)\n",
    "        }\n",
    "        \n",
    "        self.scheduler = self.scheduler_options[self.config['scheduler']]\n",
    "        \n",
    "        self.train_losses = []\n",
    "        \n",
    "    def _get_optim(self):\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "        \n",
    "\n",
    "    def train_one_epoch(self,epoch):\n",
    "        \n",
    "        running_loss = 0.\n",
    "        progress = tqdm(self.train_loader, total=len(self.train_loader))\n",
    "        true = []\n",
    "        pred = []\n",
    "        for i,(inputs,targets) in enumerate(progress):\n",
    "            self.optim.zero_grad()\n",
    "            \n",
    "            inputs = {k:inputs[k].to(device=config['device']) for k in inputs.keys()}\n",
    "            true += [int(i) for i in targets['labels']]\n",
    "            #print(true)\n",
    "            targets = targets['labels'].to(device=config['device'])\n",
    "            \n",
    "            outputs = self.model(inputs)\n",
    "            pred += [int(i) for i in torch.max(outputs.data.detach().cpu(),dim=1)[1]]\n",
    "            loss = self.loss_fn(outputs, targets)\n",
    "            running_loss += loss.item()\n",
    "            #print(pred)\n",
    "            loss.backward()\n",
    "            self.optim.step()\n",
    "            print(\"{:.2f}\".format(accuracy_score(true,pred)))\n",
    "            if self.config['scheduler'] == 'CosineAnnealingWarmRestarts':\n",
    "                self.scheduler.step(epoch-1+i/len(self.train_loader)) # as per pytorch docs\n",
    "            \n",
    "            del inputs, targets, outputs, loss\n",
    "            \n",
    "        if self.config['scheduler'] == 'StepLR':\n",
    "            self.scheduler.step()\n",
    "        \n",
    "        train_loss = running_loss/len(self.train_loader)\n",
    "        self.train_losses.append(train_loss)\n",
    "        print(accuracy_score(true,pred))\n",
    "        print(cohen_kappa_score(true,pred,weights='quadratic'))\n",
    "            \n",
    "    def test(self, test_loader):\n",
    "        \n",
    "        preds = []\n",
    "        for (inputs) in test_loader:\n",
    "            inputs = {k:inputs[k].to(device=config['device']) for k in inputs.keys()}\n",
    "            \n",
    "            outputs = self.model(inputs)\n",
    "            preds.append(outputs.detach().cpu())\n",
    "            \n",
    "        preds = torch.concat(preds)\n",
    "        return preds\n",
    "    \n",
    "    def fit(self):\n",
    "        \n",
    "        fit_progress = tqdm(\n",
    "            range(1, self.config['epochs']+1),\n",
    "            leave = True,\n",
    "            desc=\"Training...\"\n",
    "        )\n",
    "        \n",
    "        for epoch in fit_progress:\n",
    "            #print('start')\n",
    "            self.model.train()\n",
    "            fit_progress.set_description(f\"EPOCH {epoch} / {self.config['epochs']} | training...\")\n",
    "            self.train_one_epoch(epoch)\n",
    "            self.clear()\n",
    "    \n",
    "\n",
    "            print(f\"{'-'*30} EPOCH {epoch} / {self.config['epochs']} {'-'*30}\")\n",
    "            print(f\"train loss: {self.train_losses[-1]}\")\n",
    "\n",
    "            \n",
    "    \n",
    "    def clear(self):\n",
    "\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FrozenEssayModel(config).to(device=config['device'])\n",
    "trainer_freeze = Trainer(model, train_loader, config, lr='freeze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d662402aaca849558036e548c0ec7e87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f555aad6d5944f37b49b46f4213513ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44\n",
      "0.47\n",
      "0.48\n",
      "0.42\n",
      "0.42\n",
      "0.42\n",
      "0.43\n",
      "0.42\n",
      "0.40\n",
      "0.40\n",
      "0.43\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.45\n",
      "0.44\n",
      "0.44\n",
      "0.45\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.43\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.45\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.4471428571428571\n",
      "0.44683038648339235\n",
      "------------------------------ EPOCH 1 / 5 ------------------------------\n",
      "train loss: 1.6274552291089839\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c09856544484d0a9fddcbc5d554df5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38\n",
      "0.41\n",
      "0.42\n",
      "0.42\n",
      "0.40\n",
      "0.40\n",
      "0.42\n",
      "0.41\n",
      "0.40\n",
      "0.39\n",
      "0.42\n",
      "0.44\n",
      "0.45\n",
      "0.46\n",
      "0.45\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.44\n",
      "0.45\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.45\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.44\n",
      "0.45\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.44\n",
      "0.44\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.45\n",
      "0.445\n",
      "0.4603969936403931\n",
      "------------------------------ EPOCH 2 / 5 ------------------------------\n",
      "train loss: 1.5905964523553848\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "038d9a2bbd37429ca6a40388794390a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/88 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.44\n",
      "0.38\n",
      "0.40\n",
      "0.39\n",
      "0.38\n",
      "0.38\n",
      "0.38\n",
      "0.39\n",
      "0.36\n",
      "0.36\n",
      "0.39\n",
      "0.40\n",
      "0.41\n",
      "0.41\n",
      "0.41\n",
      "0.40\n",
      "0.41\n",
      "0.42\n",
      "0.42\n",
      "0.42\n",
      "0.42\n",
      "0.42\n",
      "0.42\n",
      "0.42\n",
      "0.42\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.43\n",
      "0.44\n",
      "0.44\n",
      "0.44\n",
      "0.46\n",
      "0.46\n",
      "0.45\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[61], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer_freeze\u001b[39m.\u001b[39;49mfit()\n",
      "Cell \u001b[1;32mIn[57], line 98\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m     97\u001b[0m fit_progress\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEPOCH \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m | training...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 98\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_one_epoch(epoch)\n\u001b[0;32m     99\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclear()\n\u001b[0;32m    102\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m EPOCH \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m30\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[57], line 54\u001b[0m, in \u001b[0;36mTrainer.train_one_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m     51\u001b[0m targets \u001b[39m=\u001b[39m targets[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     53\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(inputs)\n\u001b[1;32m---> 54\u001b[0m pred \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m [\u001b[39mint\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m torch\u001b[39m.\u001b[39mmax(outputs\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mcpu(),dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]]\n\u001b[0;32m     55\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_fn(outputs, targets)\n\u001b[0;32m     56\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer_freeze.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(testds,\n",
    "                                           batch_size=config['batch_size'],\n",
    "                                           shuffle=False,\n",
    "                                           num_workers=0,\n",
    "                                           pin_memory=True\n",
    "                                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = trainer_freeze.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = [list(i).index(max(i))+2 for i in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score,mean_absolute_error,mean_squared_error,accuracy_score,explained_variance_score,r2_score,confusion_matrix,ConfusionMatrixDisplay,classification_report,f1_score, silhouette_score,adjusted_mutual_info_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestY = data[data['essay_set']==1]['domain1_score'][1400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.46736292428198434\n",
      "0.5069858621797463\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(TestY,pred))\n",
    "print(cohen_kappa_score(TestY,pred,weights='quadratic'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
