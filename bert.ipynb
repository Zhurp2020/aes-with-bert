{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT embeddings extraction\n",
    "This notebook contains code to extract contextual embeddings of essays using DeBERTa\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run utils.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking CUDA availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataLoader = DataLoader()\n",
    "L2WritingData = DataLoader.GetData(source='L2Writing')\n",
    "L2WritingShuffle = DataLoader.GetShuffled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0016926B079C</td>\n",
       "      <td>I think that students would benefit from learn...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0022683E9EA5</td>\n",
       "      <td>When a problem is a change you have to let it ...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00299B378633</td>\n",
       "      <td>Dear, Principal\\n\\nIf u change the school poli...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>003885A45F42</td>\n",
       "      <td>The best time in life is when you become yours...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0049B1DF5CCC</td>\n",
       "      <td>Small act of kindness can impact in other peop...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text_id                                          full_text  cohesion  \\\n",
       "0  0016926B079C  I think that students would benefit from learn...       3.5   \n",
       "1  0022683E9EA5  When a problem is a change you have to let it ...       2.5   \n",
       "2  00299B378633  Dear, Principal\\n\\nIf u change the school poli...       3.0   \n",
       "3  003885A45F42  The best time in life is when you become yours...       4.5   \n",
       "4  0049B1DF5CCC  Small act of kindness can impact in other peop...       2.5   \n",
       "\n",
       "   syntax  vocabulary  phraseology  grammar  conventions  \n",
       "0     3.5         3.0          3.0      4.0          3.0  \n",
       "1     2.5         3.0          2.0      2.0          2.5  \n",
       "2     3.5         3.0          3.0      3.0          2.5  \n",
       "3     4.5         4.5          4.5      4.0          5.0  \n",
       "4     3.0         3.0          3.0      2.5          2.5  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2WritingData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>full_text</th>\n",
       "      <th>cohesion</th>\n",
       "      <th>syntax</th>\n",
       "      <th>vocabulary</th>\n",
       "      <th>phraseology</th>\n",
       "      <th>grammar</th>\n",
       "      <th>conventions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2177</th>\n",
       "      <td>A2957D006D28</td>\n",
       "      <td>It has been said that the first impression are...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>CE43DBA12965</td>\n",
       "      <td>The growth of technology has made it convenien...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>AEE8A576989C</td>\n",
       "      <td>Dear friend\\n\\nmy name is STUDENT_NAME\\n\\nam c...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>B5AA232A7261</td>\n",
       "      <td>Dear, Mrs. Generic_Name\\n\\nMy opinon is that i...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3409</th>\n",
       "      <td>EA37D9C12C91</td>\n",
       "      <td>life is to much fun and joy and happines we ju...</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           text_id                                          full_text  \\\n",
       "2177  A2957D006D28  It has been said that the first impression are...   \n",
       "2833  CE43DBA12965  The growth of technology has made it convenien...   \n",
       "2352  AEE8A576989C  Dear friend\\n\\nmy name is STUDENT_NAME\\n\\nam c...   \n",
       "2446  B5AA232A7261  Dear, Mrs. Generic_Name\\n\\nMy opinon is that i...   \n",
       "3409  EA37D9C12C91  life is to much fun and joy and happines we ju...   \n",
       "\n",
       "      cohesion  syntax  vocabulary  phraseology  grammar  conventions  \n",
       "2177       4.5     4.0         4.0          4.0      4.0          4.0  \n",
       "2833       3.0     3.0         3.0          3.5      3.5          4.0  \n",
       "2352       2.5     2.0         3.0          2.5      2.0          2.5  \n",
       "2446       2.0     2.0         2.0          2.0      2.0          2.0  \n",
       "3409       2.5     2.0         2.0          2.5      2.5          2.5  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2WritingShuffle.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "DebertaLargeModel = AutoModel.from_pretrained(\"microsoft/deberta-v3-large\")\n",
    "DebertaLargeTokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-large')\n",
    "#DebertaLargeModel.save_pretrained('./model/deberta-v3-large/')\n",
    "#DebertaLargeTokenizer.save_pretrained('/model/tokenizer/deberta-v3-large/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-pegasus-large-arxiv were not used when initializing BigBirdPegasusModel: ['final_logits_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing BigBirdPegasusModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdPegasusModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "BigBirdModel = AutoModel.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "BigBirdTokenizer = AutoTokenizer.from_pretrained('google/bigbird-pegasus-large-arxiv')\n",
    "#BigBirdModel.save_pretrained('./model/bigbird-large')\n",
    "#BigBirdTokenizer.save_pretrained('./model/tokenizer/bigbird-large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b00e082a33384ed7be5975fac01589a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/579 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GIGA\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b62a7b4b26c4f328d306f2aaf98ce02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/371M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59223db47eaf4aa180054c5c0e571b07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4962ac96836547daa6ac3b5eda6c0851",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.46M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer/deberta-v3-base\\\\tokenizer_config.json',\n",
       " './model/tokenizer/deberta-v3-base\\\\special_tokens_map.json',\n",
       " './model/tokenizer/deberta-v3-base\\\\spm.model',\n",
       " './model/tokenizer/deberta-v3-base\\\\added_tokens.json',\n",
       " './model/tokenizer/deberta-v3-base\\\\tokenizer.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debertabaseModel = AutoModel.from_pretrained('microsoft/deberta-v3-base')\n",
    "debertabaseTokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "debertabaseModel.save_pretrained('./model/deberta-v3-base')\n",
    "debertabaseTokenizer.save_pretrained('./model/tokenizer/deberta-v3-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbec5930e0524383ac799406b5e79762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\GIGA\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbebd3713b4e46b285c93f8314f3a622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./model/tokenizer/deberta-base\\\\tokenizer_config.json',\n",
       " './model/tokenizer/deberta-base\\\\special_tokens_map.json',\n",
       " './model/tokenizer/deberta-base\\\\vocab.json',\n",
       " './model/tokenizer/deberta-base\\\\merges.txt',\n",
       " './model/tokenizer/deberta-base\\\\added_tokens.json',\n",
       " './model/tokenizer/deberta-base\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deberta = AutoModel.from_pretrained('microsoft/deberta-base')\n",
    "debertatokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-base')\n",
    "deberta.save_pretrained('./model/deberta-base')\n",
    "debertatokenizer.save_pretrained('./model/tokenizer/deberta-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIGA\\AppData\\Local\\Temp\\ipykernel_26408\\857485619.py:1: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  data = L2WritingShuffle['full_text'][:5]\n"
     ]
    }
   ],
   "source": [
    "data = L2WritingShuffle['full_text'][:5]\n",
    "tokenized = [DebertaLargeTokenizer(\n",
    "    i,\n",
    "    add_special_tokens=True,\n",
    "    max_length=512,\n",
    "    return_tensors='pt',\n",
    "    padding='max_length',\n",
    "    truncation=True\n",
    "    ).to('cuda') for i in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DebertaLargeModel= DebertaLargeModel.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = DebertaLargeModel(tokenized[0]['input_ids'],tokenized[0]['attention_mask'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get embeddings\n",
    "### CLS embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.0279e-01, -6.3384e-02,  1.3956e-03,  ...,  6.9370e-03,\n",
       "         -5.7944e+00,  4.2127e-02]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden = out.last_hidden_state.detach().cpu()\n",
    "CLSEmbedding = hidden[:,0]\n",
    "print(CLSEmbedding.shape)\n",
    "CLSEmbedding[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4959, -0.2764, -0.0538,  ...,  0.3599, -0.3727,  0.3367]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttentionMask = tokenized[0]['attention_mask'].detach().cpu().unsqueeze(-1).expand(hidden.size())\n",
    "SumEmbeddings = torch.sum(AttentionMask*hidden,1)\n",
    "SumMask = AttentionMask.sum(1)\n",
    "SumMask = SumMask.clamp(min=1e-9)\n",
    "MeanPooling = SumEmbeddings/SumMask\n",
    "print(MeanPooling.shape)\n",
    "MeanPooling[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1024])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1.8355, 2.8901, 1.5904,  ..., 2.3680, 1.6260, 1.8040]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AttentionMask = tokenized[0]['attention_mask'].detach().cpu().unsqueeze(-1).expand(hidden.size())\n",
    "hidden[AttentionMask == 0] = -1e9\n",
    "MaxPooling = torch.max(hidden,1)[0]\n",
    "print(MaxPooling.shape)\n",
    "MaxPooling[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetBERTEmbeddings():\n",
    "    def __init__(self,input,model):\n",
    "        # Check input\n",
    "        if model in ['microsoft/deberta-v3-large','google/bigbird-pegasus-large-arxiv']:\n",
    "            self.input = input\n",
    "            # load model and tokenizer\n",
    "            self.model = AutoModel.from_pretrained(model)\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "            self.hidden = []\n",
    "        else:\n",
    "            raise KeyError\n",
    "    def tokenize(self,SeqLen=1024):\n",
    "        self.tokenized = []\n",
    "        for seq in self.input:\n",
    "            self.tokenized.append(\n",
    "                self.tokenizer(seq,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=SeqLen,    # max sequence length, default is 512 for deberta-v3-large\n",
    "                    return_tensors='pt',  # return in tensor\n",
    "                    padding='max_length',\n",
    "                    truncation=True))\n",
    "        self.input = self.tokenized # move to gpu\n",
    "        self.input = [i.to('cuda') for i in self.input]\n",
    "    def inf(self,stop=1000,SeqLen = 1024,):\n",
    "        self.tokenize(SeqLen=SeqLen)\n",
    "        print('tokenized')\n",
    "        self.model = self.model.to('cuda') # move to gpu\n",
    "        for run in range(len(self.input) // stop):\n",
    "            for i in range(stop):\n",
    "                with torch.no_grad():\n",
    "                    out = self.model(self.input[run+i]['input_ids'],self.input[run+i]['attention_mask']) # inference\n",
    "                self.hidden.append(out.last_hidden_state.detach().cpu()) # detach to cpu\n",
    "                if i % 10 == 0:\n",
    "                    print('{}/{}, run:{}'.format(i,stop,run))\n",
    "                del out \n",
    "            torch.cuda.empty_cache() # clear cuda memory for next run\n",
    "        #if len(self.input) > stop: # remaining ones\n",
    "        t = len(self.input) // stop * stop\n",
    "        for i in range(t,len(self.input)):\n",
    "            with torch.no_grad():\n",
    "                out = self.model(self.input[i]['input_ids'],self.input[i]['attention_mask']) # inference\n",
    "            self.hidden.append(out.last_hidden_state.detach().cpu()) # detach to cpu\n",
    "            if i % 10 == 0:\n",
    "                print('{}/{}, run:{}'.format(i,stop,'f'))\n",
    "            del out \n",
    "        torch.cuda.empty_cache()\n",
    "    def CLSEmbedding(self,i):\n",
    "        self.CLS = self.hidden[i][:,0]\n",
    "        return self.CLS\n",
    "    def MaxPooling(self,i):\n",
    "        hidden = self.hidden[i]\n",
    "        self.AttentionMask = self.tokenized[i]['attention_mask'].unsqueeze(-1).expand(hidden.size())\n",
    "        hidden[self.AttentionMask == 0] = -1e9 # ignore paddings\n",
    "        self.MaxP = torch.max(hidden,1)[0]\n",
    "        return self.MaxP\n",
    "    def MeanPooling(self,i):\n",
    "        hidden = self.hidden[i].detach().cpu()\n",
    "        self.AttentionMask = self.tokenized[i]['attention_mask'].unsqueeze(-1).expand(hidden.size()).detach().cpu()\n",
    "        SumEmbeddings = torch.sum(self.AttentionMask*hidden,1) # ignore paddings\n",
    "        SumMask = self.AttentionMask.sum(1)\n",
    "        SumMask = SumMask.clamp(min=1e-9) # prevents division by zero\n",
    "        self.MeanP = SumEmbeddings/SumMask\n",
    "        return self.MeanP\n",
    "    def GetEmbeddings(self,type) :\n",
    "        EmbeddingType = {\n",
    "            'CLS':self.CLSEmbedding,\n",
    "            'MaxP':self.MaxPooling,\n",
    "            'MeanP':self.MeanPooling\n",
    "        }\n",
    "        result = [EmbeddingType[type](i) for i in range(len(self.input)) ]\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GIGA\\AppData\\Local\\Temp\\ipykernel_17436\\3811778809.py:1: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  EmbeddingFetcher = GetBERTEmbeddings(L2WritingShuffle['full_text'][:50],'google/bigbird-pegasus-large-arxiv')\n",
      "Some weights of the model checkpoint at google/bigbird-pegasus-large-arxiv were not used when initializing BigBirdPegasusModel: ['final_logits_bias', 'lm_head.weight']\n",
      "- This IS expected if you are initializing BigBirdPegasusModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdPegasusModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bigbird_pegasus\\modeling_bigbird_pegasus.py:807: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  torch.arange(indices.shape[0] * indices.shape[1] * num_indices_to_gather, device=indices.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1000, run:f\n",
      "10/1000, run:f\n",
      "20/1000, run:f\n",
      "30/1000, run:f\n",
      "40/1000, run:f\n"
     ]
    }
   ],
   "source": [
    "EmbeddingFetcher = GetBERTEmbeddings(L2WritingShuffle['full_text'][:50],'google/bigbird-pegasus-large-arxiv')\n",
    "EmbeddingFetcher.inf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.1633, -0.1339,  0.2622,  ...,  0.0476,  0.0208,  0.2765]]),\n",
       " tensor([[ 0.0298, -0.1334,  0.2221,  ...,  0.1089, -0.0831,  0.1603]]),\n",
       " tensor([[-0.0224, -0.1506,  0.1303,  ...,  0.0075, -0.0181,  0.0877]]),\n",
       " tensor([[ 0.0705, -0.0944,  0.2164,  ...,  0.0218,  0.0009,  0.1355]]),\n",
       " tensor([[-0.0431, -0.1462,  0.1437,  ...,  0.0270, -0.0045,  0.0694]]),\n",
       " tensor([[-0.0301, -0.1147,  0.1494,  ...,  0.0416, -0.0227,  0.0984]]),\n",
       " tensor([[-0.1737, -0.1156,  0.2163,  ...,  0.0438, -0.0238,  0.2262]]),\n",
       " tensor([[ 0.0035, -0.1286,  0.1948,  ..., -0.0114, -0.0364,  0.1432]]),\n",
       " tensor([[-0.0038, -0.1254,  0.2023,  ...,  0.0964,  0.0020,  0.2621]]),\n",
       " tensor([[-0.0493, -0.1919,  0.2049,  ...,  0.1426, -0.0006,  0.1389]]),\n",
       " tensor([[-0.0382, -0.1667,  0.1721,  ...,  0.0406, -0.0415,  0.0773]]),\n",
       " tensor([[ 0.0833, -0.1503,  0.0911,  ...,  0.0608, -0.0191,  0.0564]]),\n",
       " tensor([[ 0.0351, -0.1271,  0.1517,  ...,  0.1259,  0.0118,  0.1516]]),\n",
       " tensor([[ 0.0122, -0.1490,  0.1924,  ...,  0.0427, -0.0712,  0.0531]]),\n",
       " tensor([[ 0.0774, -0.1534,  0.2876,  ...,  0.0143, -0.0256,  0.1115]]),\n",
       " tensor([[ 0.0070, -0.1613,  0.1261,  ...,  0.0686, -0.0101,  0.1739]]),\n",
       " tensor([[ 0.0039, -0.1218,  0.2224,  ...,  0.1255, -0.0634,  0.1419]]),\n",
       " tensor([[ 0.0310, -0.1071,  0.2192,  ...,  0.1237, -0.0603,  0.1810]]),\n",
       " tensor([[ 0.0158, -0.1816,  0.1783,  ...,  0.0313, -0.0397,  0.0592]]),\n",
       " tensor([[-0.0427, -0.1562,  0.2364,  ...,  0.0482, -0.0692,  0.1352]]),\n",
       " tensor([[ 0.0083, -0.0965,  0.2919,  ...,  0.0397, -0.0919,  0.1336]]),\n",
       " tensor([[ 0.0608, -0.1663,  0.1518,  ...,  0.0402, -0.0261,  0.0858]]),\n",
       " tensor([[-0.0727, -0.0925,  0.2474,  ...,  0.0250, -0.0192,  0.0906]]),\n",
       " tensor([[-0.0414, -0.1352,  0.1762,  ...,  0.1214, -0.0833,  0.0939]]),\n",
       " tensor([[ 0.2451, -0.2006,  0.1666,  ...,  0.1882, -0.0085,  0.0243]]),\n",
       " tensor([[ 0.0369, -0.1098,  0.2943,  ...,  0.0403, -0.0252,  0.1770]]),\n",
       " tensor([[ 0.0292, -0.1319,  0.1687,  ...,  0.0901, -0.0478,  0.0732]]),\n",
       " tensor([[-0.0942, -0.1762,  0.2633,  ...,  0.1823,  0.0331,  0.0888]]),\n",
       " tensor([[ 0.0103, -0.1661,  0.1560,  ...,  0.0947,  0.0218,  0.1342]]),\n",
       " tensor([[ 0.0053, -0.1765,  0.2018,  ...,  0.1335, -0.0881,  0.2036]]),\n",
       " tensor([[-0.0785, -0.1684,  0.2701,  ...,  0.0726, -0.0284,  0.0746]]),\n",
       " tensor([[ 0.0494, -0.1442,  0.1094,  ...,  0.0113, -0.0307,  0.1466]]),\n",
       " tensor([[-0.0638, -0.1312,  0.2210,  ...,  0.0755,  0.0657,  0.1248]]),\n",
       " tensor([[-0.1485, -0.1525,  0.2513,  ...,  0.0502, -0.0633,  0.1089]]),\n",
       " tensor([[ 0.0704, -0.1145,  0.1653,  ...,  0.0367, -0.0515,  0.1555]]),\n",
       " tensor([[ 0.0230, -0.1377,  0.2578,  ...,  0.0684, -0.0818,  0.0926]]),\n",
       " tensor([[ 0.1640, -0.1231,  0.2266,  ...,  0.0685,  0.0381,  0.1342]]),\n",
       " tensor([[-0.0560, -0.1313,  0.2080,  ...,  0.0206, -0.0818,  0.1536]]),\n",
       " tensor([[ 0.0169, -0.1202,  0.1607,  ...,  0.1242, -0.0043,  0.2007]]),\n",
       " tensor([[-0.0187, -0.1312,  0.2183,  ...,  0.0718, -0.0749,  0.0737]]),\n",
       " tensor([[-0.0159, -0.1696,  0.2061,  ...,  0.0487,  0.0012,  0.0909]]),\n",
       " tensor([[ 0.0891, -0.1351,  0.2169,  ...,  0.0378, -0.0303,  0.0864]]),\n",
       " tensor([[-0.0712, -0.1764,  0.2409,  ..., -0.0031, -0.0442,  0.0778]]),\n",
       " tensor([[ 0.0223, -0.1557,  0.2202,  ...,  0.0147, -0.0256,  0.0321]]),\n",
       " tensor([[-0.0835, -0.1003,  0.2681,  ...,  0.0846, -0.0549,  0.1600]]),\n",
       " tensor([[ 0.0196, -0.1518,  0.1945,  ...,  0.0759,  0.0247,  0.1360]]),\n",
       " tensor([[ 0.0149, -0.1874,  0.1260,  ...,  0.0554, -0.1073,  0.0779]]),\n",
       " tensor([[-0.0093, -0.1265,  0.1773,  ...,  0.0256, -0.0676,  0.0973]]),\n",
       " tensor([[ 0.0078, -0.1341,  0.1447,  ...,  0.0466,  0.0234,  0.1631]]),\n",
       " tensor([[ 0.0479, -0.1386,  0.2338,  ...,  0.1143, -0.0660,  0.1062]])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EmbeddingFetcher.GetEmbeddings('MeanP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "593988f55108e9a22df825be697685e2e60f0c546ce2b7da78c95a16021f878c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
