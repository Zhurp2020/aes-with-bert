{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run bert\n",
    "%run utils.ipynb\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as sk \n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import cohen_kappa_score,mean_absolute_error,mean_squared_error,accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>rater1_domain1</th>\n",
       "      <th>rater2_domain1</th>\n",
       "      <th>rater3_domain1</th>\n",
       "      <th>domain1_score</th>\n",
       "      <th>rater1_domain2</th>\n",
       "      <th>rater2_domain2</th>\n",
       "      <th>domain2_score</th>\n",
       "      <th>...</th>\n",
       "      <th>rater2_trait3</th>\n",
       "      <th>rater2_trait4</th>\n",
       "      <th>rater2_trait5</th>\n",
       "      <th>rater2_trait6</th>\n",
       "      <th>rater3_trait1</th>\n",
       "      <th>rater3_trait2</th>\n",
       "      <th>rater3_trait3</th>\n",
       "      <th>rater3_trait4</th>\n",
       "      <th>rater3_trait5</th>\n",
       "      <th>rater3_trait6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   essay_id  essay_set                                              essay  \\\n",
       "0         1          1  Dear local newspaper, I think effects computer...   \n",
       "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "\n",
       "   rater1_domain1  rater2_domain1  rater3_domain1  domain1_score  \\\n",
       "0             4.0             4.0             NaN            8.0   \n",
       "1             5.0             4.0             NaN            9.0   \n",
       "2             4.0             3.0             NaN            7.0   \n",
       "3             5.0             5.0             NaN           10.0   \n",
       "4             4.0             4.0             NaN            8.0   \n",
       "\n",
       "   rater1_domain2  rater2_domain2  domain2_score  ...  rater2_trait3  \\\n",
       "0             NaN             NaN            NaN  ...            NaN   \n",
       "1             NaN             NaN            NaN  ...            NaN   \n",
       "2             NaN             NaN            NaN  ...            NaN   \n",
       "3             NaN             NaN            NaN  ...            NaN   \n",
       "4             NaN             NaN            NaN  ...            NaN   \n",
       "\n",
       "   rater2_trait4  rater2_trait5  rater2_trait6  rater3_trait1  rater3_trait2  \\\n",
       "0            NaN            NaN            NaN            NaN            NaN   \n",
       "1            NaN            NaN            NaN            NaN            NaN   \n",
       "2            NaN            NaN            NaN            NaN            NaN   \n",
       "3            NaN            NaN            NaN            NaN            NaN   \n",
       "4            NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "   rater3_trait3  rater3_trait4  rater3_trait5  rater3_trait6  \n",
       "0            NaN            NaN            NaN            NaN  \n",
       "1            NaN            NaN            NaN            NaN  \n",
       "2            NaN            NaN            NaN            NaN  \n",
       "3            NaN            NaN            NaN            NaN  \n",
       "4            NaN            NaN            NaN            NaN  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loader = DataLoader()\n",
    "d = loader.GetData('ASAP')\n",
    "d = d.loc[d['essay_set'] ==1]\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    # Remove punctuation\n",
    "    text = re.sub(\"[%s]\" % re.escape(string.punctuation), \"\", text)\n",
    "    # Remove non-Roman characters\n",
    "    text = re.sub(\"([^\\x00-\\x7F])+\", \" \", text)    \n",
    "    text = re.sub(\"[\\n\\r\\t]\", \" \", text)   \n",
    "    return text\n",
    "t = [clean_text(i) for i in d['essay']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:745: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_scores = torch.bmm(query_layer, key_layer.transpose(-1, -2)) / torch.tensor(\n",
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:829: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += c2p_att / torch.tensor(scale, dtype=c2p_att.dtype)\n",
      "c:\\Users\\GIGA\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\deberta_v2\\modeling_deberta_v2.py:852: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  score += p2c_att / torch.tensor(scale, dtype=p2c_att.dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1500, run:0\n",
      "10/1500, run:0\n",
      "20/1500, run:0\n",
      "30/1500, run:0\n",
      "40/1500, run:0\n",
      "50/1500, run:0\n",
      "60/1500, run:0\n",
      "70/1500, run:0\n",
      "80/1500, run:0\n",
      "90/1500, run:0\n",
      "100/1500, run:0\n",
      "110/1500, run:0\n",
      "120/1500, run:0\n",
      "130/1500, run:0\n",
      "140/1500, run:0\n",
      "150/1500, run:0\n",
      "160/1500, run:0\n",
      "170/1500, run:0\n",
      "180/1500, run:0\n",
      "190/1500, run:0\n",
      "200/1500, run:0\n",
      "210/1500, run:0\n",
      "220/1500, run:0\n",
      "230/1500, run:0\n",
      "240/1500, run:0\n",
      "250/1500, run:0\n",
      "260/1500, run:0\n",
      "270/1500, run:0\n",
      "280/1500, run:0\n",
      "290/1500, run:0\n",
      "300/1500, run:0\n",
      "310/1500, run:0\n",
      "320/1500, run:0\n",
      "330/1500, run:0\n",
      "340/1500, run:0\n",
      "350/1500, run:0\n",
      "360/1500, run:0\n",
      "370/1500, run:0\n",
      "380/1500, run:0\n",
      "390/1500, run:0\n",
      "400/1500, run:0\n",
      "410/1500, run:0\n",
      "420/1500, run:0\n",
      "430/1500, run:0\n",
      "440/1500, run:0\n",
      "450/1500, run:0\n",
      "460/1500, run:0\n",
      "470/1500, run:0\n",
      "480/1500, run:0\n",
      "490/1500, run:0\n",
      "500/1500, run:0\n",
      "510/1500, run:0\n",
      "520/1500, run:0\n",
      "530/1500, run:0\n",
      "540/1500, run:0\n",
      "550/1500, run:0\n",
      "560/1500, run:0\n",
      "570/1500, run:0\n",
      "580/1500, run:0\n",
      "590/1500, run:0\n",
      "600/1500, run:0\n",
      "610/1500, run:0\n",
      "620/1500, run:0\n",
      "630/1500, run:0\n",
      "640/1500, run:0\n",
      "650/1500, run:0\n",
      "660/1500, run:0\n",
      "670/1500, run:0\n",
      "680/1500, run:0\n",
      "690/1500, run:0\n",
      "700/1500, run:0\n",
      "710/1500, run:0\n",
      "720/1500, run:0\n",
      "730/1500, run:0\n",
      "740/1500, run:0\n",
      "750/1500, run:0\n",
      "760/1500, run:0\n",
      "770/1500, run:0\n",
      "780/1500, run:0\n",
      "790/1500, run:0\n",
      "800/1500, run:0\n",
      "810/1500, run:0\n",
      "820/1500, run:0\n",
      "830/1500, run:0\n",
      "840/1500, run:0\n",
      "850/1500, run:0\n",
      "860/1500, run:0\n",
      "870/1500, run:0\n",
      "880/1500, run:0\n",
      "890/1500, run:0\n",
      "900/1500, run:0\n",
      "910/1500, run:0\n",
      "920/1500, run:0\n",
      "930/1500, run:0\n",
      "940/1500, run:0\n",
      "950/1500, run:0\n",
      "960/1500, run:0\n",
      "970/1500, run:0\n",
      "980/1500, run:0\n",
      "990/1500, run:0\n",
      "1000/1500, run:0\n",
      "1010/1500, run:0\n",
      "1020/1500, run:0\n",
      "1030/1500, run:0\n",
      "1040/1500, run:0\n",
      "1050/1500, run:0\n",
      "1060/1500, run:0\n",
      "1070/1500, run:0\n",
      "1080/1500, run:0\n",
      "1090/1500, run:0\n",
      "1100/1500, run:0\n",
      "1110/1500, run:0\n",
      "1120/1500, run:0\n",
      "1130/1500, run:0\n",
      "1140/1500, run:0\n",
      "1150/1500, run:0\n",
      "1160/1500, run:0\n",
      "1170/1500, run:0\n",
      "1180/1500, run:0\n",
      "1190/1500, run:0\n",
      "1200/1500, run:0\n",
      "1210/1500, run:0\n",
      "1220/1500, run:0\n",
      "1230/1500, run:0\n",
      "1240/1500, run:0\n",
      "1250/1500, run:0\n",
      "1260/1500, run:0\n",
      "1270/1500, run:0\n",
      "1280/1500, run:0\n",
      "1290/1500, run:0\n",
      "1300/1500, run:0\n",
      "1310/1500, run:0\n",
      "1320/1500, run:0\n",
      "1330/1500, run:0\n",
      "1340/1500, run:0\n",
      "1350/1500, run:0\n",
      "1360/1500, run:0\n",
      "1370/1500, run:0\n",
      "1380/1500, run:0\n",
      "1390/1500, run:0\n",
      "1400/1500, run:0\n",
      "1410/1500, run:0\n",
      "1420/1500, run:0\n",
      "1430/1500, run:0\n",
      "1440/1500, run:0\n",
      "1450/1500, run:0\n",
      "1460/1500, run:0\n",
      "1470/1500, run:0\n",
      "1480/1500, run:0\n",
      "1490/1500, run:0\n",
      "1500/1500, run:f\n",
      "1510/1500, run:f\n",
      "1520/1500, run:f\n",
      "1530/1500, run:f\n",
      "1540/1500, run:f\n",
      "1550/1500, run:f\n",
      "1560/1500, run:f\n",
      "1570/1500, run:f\n",
      "1580/1500, run:f\n",
      "1590/1500, run:f\n",
      "1600/1500, run:f\n",
      "1610/1500, run:f\n",
      "1620/1500, run:f\n",
      "1630/1500, run:f\n",
      "1640/1500, run:f\n",
      "1650/1500, run:f\n",
      "1660/1500, run:f\n",
      "1670/1500, run:f\n",
      "1680/1500, run:f\n",
      "1690/1500, run:f\n",
      "1700/1500, run:f\n",
      "1710/1500, run:f\n",
      "1720/1500, run:f\n",
      "1730/1500, run:f\n",
      "1740/1500, run:f\n",
      "1750/1500, run:f\n",
      "1760/1500, run:f\n",
      "1770/1500, run:f\n",
      "1780/1500, run:f\n"
     ]
    }
   ],
   "source": [
    "EmbeddingFetcher = GetBERTEmbeddings(d['essay'],'model/deberta-v3-large')\n",
    "EmbeddingFetcher.inf(stop=1500,SeqLen = 512 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "del EmbeddingFetcher.model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = EmbeddingFetcher.GetEmbeddings('CLS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [np.array(i).reshape(1024) for i in x]\n",
    "TrainX,TestX = np.array(x[:1200]),np.array(x[1200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(d['domain1_score'])\n",
    "y = le.transform(d['domain1_score']) \n",
    "TrainY,TestY = y[:1200],y[1200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_class=11, num_parallel_tree=1,\n",
       "              objective=&#x27;multi:softmax&#x27;, predictor=&#x27;auto&#x27;, random_state=0, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=0.5, booster=&#x27;gbtree&#x27;, callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy=&#x27;depthwise&#x27;,\n",
       "              importance_type=None, interaction_constraints=&#x27;&#x27;,\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints=&#x27;()&#x27;, n_estimators=100,\n",
       "              n_jobs=0, num_class=11, num_parallel_tree=1,\n",
       "              objective=&#x27;multi:softmax&#x27;, predictor=&#x27;auto&#x27;, random_state=0, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.300000012, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=6, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_class=11, num_parallel_tree=1,\n",
       "              objective='multi:softmax', predictor='auto', random_state=0, ...)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = xgb.XGBClassifier(tree_method=\"hist\",objective='multi:softmax',num_class=11)\n",
    "model.fit(TrainX,TrainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "PredY = model.predict(TestX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4854202401372213"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(TestY,PredY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7619823173569101"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohen_kappa_score(TestY,PredY,weights='quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.48713550600343053, 0.2988250531964105)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x = EmbeddingFetcher.GetEmbeddings('CLS')\n",
    "#x = [np.array(i).reshape(768) for i in x]\n",
    "#TrainX,TestX = x[:2800],x[2800:]\n",
    "def objective():\n",
    "    params = {\"random_state\":42,          \n",
    "        'learning_rate' : 0.05,   \n",
    "        \"n_estimators\": 1000,                 \n",
    "        \"max_depth\" : 8,\n",
    "        \"alpha\" : 0.95,\n",
    "        \"tree_method\": 'gpu_hist', \n",
    "        \"predictor\": \"gpu_predictor\",\n",
    "        \"objective\":'multi:softmax',\n",
    "        \"num_class\":11\n",
    "    }\n",
    "\n",
    "    model = xgb.XGBClassifier(**params)\n",
    "    model.fit(TrainX,TrainY)\n",
    "    PredY = model.predict(TestX)\n",
    "    e = accuracy_score(TestY,PredY)\n",
    "    k = cohen_kappa_score(TestY,PredY)\n",
    "    return e,k\n",
    "objective()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "593988f55108e9a22df825be697685e2e60f0c546ce2b7da78c95a16021f878c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
